{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "First we need to load the testing datasets for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets from the folder\n",
    "data_dir = '../data/'\n",
    "image_sizes = ['4-4', '8-8', '16-16', '25-25', '32-32']\n",
    "dirs = [data_dir + size + '/temp_class/test/' for size in image_sizes]\n",
    "\n",
    "# define the transforms\n",
    "dset_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# load the datasets\n",
    "dsets = {y: datasets.ImageFolder(x, dset_transform) for x in dirs for y in image_sizes}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models/'\n",
    "model_names = [size + '-NN.pth' for size in image_sizes]\n",
    "model_paths = [model_dir + name for name in model_names]\n",
    "\n",
    "# load the models\n",
    "models = {y: torch.load(x) for x in model_paths for y in image_sizes}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Performance of Different Models\n",
    "\n",
    "First we define some helper functions to format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data):\n",
    "    temp_pred_below = []\n",
    "    temp_pred_above = []\n",
    "    model.eval()\n",
    "    for idx in range(len(data)):\n",
    "        image, label = data[idx]\n",
    "        predicted_output = model(image.unsqueeze(0))\n",
    "\n",
    "        # Need to apply softmax to get probabilities\n",
    "        prob_output = torch.nn.functional.softmax(predicted_output).detach().numpy()\n",
    "        predicted_label = torch.argmax(predicted_output).item()\n",
    "        # print(f\"predicted label: {predicted_label}, actual label: {label}\")\n",
    "\n",
    "        #get temp from label\n",
    "        temp = data.classes[label]\n",
    "        temp_pred_below.append([float(temp), prob_output[0][1]])\n",
    "        temp_pred_above.append([float(temp), prob_output[0][0]])\n",
    "    \n",
    "    return temp_pred_below, temp_pred_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_err(temp_pred):\n",
    "    temp_pred_avg = {}\n",
    "\n",
    "    temp_pred_std = {}\n",
    "\n",
    "    for temp, pred in temp_pred:\n",
    "        if temp in temp_pred_avg:\n",
    "            temp_pred_avg[temp].append(pred)\n",
    "        else:\n",
    "            temp_pred_avg[temp] = [pred]\n",
    "\n",
    "        if temp in temp_pred_std:\n",
    "            temp_pred_std[temp].append(pred)\n",
    "        else:\n",
    "            temp_pred_std[temp] = [pred]\n",
    "\n",
    "\n",
    "    temp_pred_avg = {temp: np.mean(pred) for temp, pred in temp_pred_avg.items()}\n",
    "\n",
    "    temp_pred_err = {temp: np.std(pred) for temp, pred in temp_pred_std.items()}\n",
    "\n",
    "    return temp_pred_avg, temp_pred_err"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# get predictions for each model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m temp_pred \u001b[39m=\u001b[39m {y: get_predictions(models[y], dsets[y]) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m image_sizes}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#get the average and error for each model and plot it\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m image_sizes:\n",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb Cell 11\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# get predictions for each model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m temp_pred \u001b[39m=\u001b[39m {y: get_predictions(models[y], dsets[y]) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m image_sizes}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#get the average and error for each model and plot it\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m image_sizes:\n",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb Cell 11\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m temp_pred_below \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m temp_pred_above \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Model_Compare.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     image, label \u001b[39m=\u001b[39m data[idx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# get predictions for each model\n",
    "temp_pred = {y: get_predictions(models[y], dsets[y]) for y in image_sizes}\n",
    "\n",
    "#get the average and error for each model and plot it\n",
    "for size in image_sizes:\n",
    "    temp_pred_below, temp_pred_above = temp_pred[size]\n",
    "    temp_pred_below_avg, temp_pred_below_err = get_avg_err(temp_pred_below)\n",
    "    temp_pred_above_avg, temp_pred_above_err = get_avg_err(temp_pred_above)\n",
    "\n",
    "    plt.errorbar(temp_pred_below_avg.keys(), temp_pred_below_avg.values(), yerr=temp_pred_below_err.values(), label='Below')\n",
    "    plt.errorbar(temp_pred_above_avg.keys(), temp_pred_above_avg.values(), yerr=temp_pred_above_err.values(), label='Above')\n",
    "    plt.title(f'Predictions for {size}x{size} images')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
