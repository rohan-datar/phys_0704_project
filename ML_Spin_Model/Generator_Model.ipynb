{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spin Configuration Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "You can load different datasets by changing the dataset path for testing, trainging, and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to the appropriate directory paths\n",
    "\n",
    "test_dir = '../data/{}-{}/binary_class/test/'.format(L,L)\n",
    "train_dir = '../data/{}-{}/binary_class/train/'.format(L,L)\n",
    "\n",
    "# Set these variables to the appropriate directory paths\n",
    "benchmark_dir = '../data/{}-{}/temp_class/test/'.format(L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms\n",
    "dset_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "#create datasets using ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=dset_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=dset_transform)\n",
    "benchmark_data = datasets.ImageFolder(benchmark_dir, transform=dset_transform)\n",
    "\n",
    "batch_size = 64\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Thanks to Teal Witter for code for these helper functions https://github.com/rtealwitter/dl-demos/blob/b537a5dd94953ea656a2140227af78f67c042540/demo11-conditional-gan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=2):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, DEVICE, n_noise=100):\n",
    "    img = np.zeros([L*10, L*10])\n",
    "    for j in range(10):\n",
    "        c = torch.zeros([10, 10]).to(DEVICE)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(10, n_noise).to(DEVICE)\n",
    "        y_hat = G(z,c).view(10, L, L)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*L:(j+1)*L] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [L,L]\n",
    "hidden_layers = 4\n",
    "layer_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Define the architecture of the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(layer_size, num_layers):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(layer_size, layer_size))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes, input_size=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(input_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, self.image_size), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(self.image_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, 1), nn.Sigmoid()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/{}-{}-GAN/'.format(L,L)\n",
    "SAMPLE_PATH = '../generated_samples/{}-{}-GAN/'.format(L,L)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "G = Generator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "\n",
    "max_epoch = 50\n",
    "step = 0\n",
    "n_noise = 100 # noise dimension\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# We will denote real images as 1s and fake images as 0s\n",
    "# This is why we needed to drop the last batch of the data loader\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, D_loss: 1.3873517513275146, G_loss: -0.7007609605789185\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x110 and 102x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     G\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     img \u001b[39m=\u001b[39m get_sample_image(G, DEVICE, n_noise)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     plt\u001b[39m.\u001b[39mimsave(SAMPLE_PATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39msample-\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, step), img, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     G\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb Cell 17\u001b[0m in \u001b[0;36mget_sample_image\u001b[0;34m(G, DEVICE, n_noise)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m c[:, j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m, n_noise)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m y_hat \u001b[39m=\u001b[39m G(z,c)\u001b[39m.\u001b[39mview(\u001b[39m10\u001b[39m, L, L)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img[j\u001b[39m*\u001b[39mL:(j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mL] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m result], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb Cell 17\u001b[0m in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m x, c \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), c\u001b[39m.\u001b[39mview(c\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, c], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcurie.middlebury.edu/home/rdatar/Senior_Project/ML_Spin_Model/Generator_Model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(v)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x110 and 102x128)"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAMPLE_PATH):\n",
    "    os.makedirs(SAMPLE_PATH)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Train Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        y = labels.view(batch_size, 1) # add singleton dimension so batch_size x 1\n",
    "        y = to_onehot(y, num_classes=2).to(DEVICE)\n",
    "\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        G_loss = -1 * criterion(z_outputs, D_fakes)\n",
    "\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: {}, Step: {}, D_loss: {}, G_loss: {}'.format(epoch, step, D_loss.item(), G_loss.item()))\n",
    "            \n",
    "        if step % 500 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, DEVICE, n_noise)\n",
    "            plt.imsave(SAMPLE_PATH + 'sample-{}-{}.png'.format(epoch, step), img, cmap='gray')\n",
    "            G.train()\n",
    "\n",
    "        step += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
