{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spin Configuration Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "You can load different datasets by changing the dataset path for testing, trainging, and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to the appropriate directory paths\n",
    "\n",
    "train_dir = '../data/{}-{}/temp_class/train/'.format(L,L)\n",
    "\n",
    "# Set these variables to the appropriate directory paths\n",
    "benchmark_dir = '../data/{}-{}/temp_class/test/'.format(L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms\n",
    "dset_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "#create datasets using ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=dset_transform)\n",
    "# test_data = datasets.ImageFolder(test_dir, transform=dset_transform)\n",
    "benchmark_data = datasets.ImageFolder(benchmark_dir, transform=dset_transform)\n",
    "\n",
    "batch_size = 64\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_data.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Thanks to Teal Witter for code for these helper functions https://github.com/rtealwitter/dl-demos/blob/b537a5dd94953ea656a2140227af78f67c042540/demo11-conditional-gan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_generator_output(x):\n",
    "    #convert to binary data\n",
    "    x[x>=0.5] = 1\n",
    "    x[x<0.5] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=2):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, DEVICE, n_noise=100):\n",
    "    img = np.zeros([L*num_classes, L*num_classes])\n",
    "    for j in range(num_classes):\n",
    "        c = torch.zeros([num_classes, num_classes]).to(DEVICE)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(num_classes, n_noise).to(DEVICE)\n",
    "        y_hat = G(z,c).view(num_classes, L, L)\n",
    "        y_hat = normalize_generator_output(y_hat)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*L:(j+1)*L] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [L,L]\n",
    "hidden_layers = 4\n",
    "layer_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Define the architecture of the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(layer_size, num_layers):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(layer_size, layer_size))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes, input_size=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(input_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, self.image_size), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(self.image_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, 1), nn.Sigmoid()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/{}-{}-GAN/'.format(L,L)\n",
    "SAMPLE_PATH = '../generated_samples/{}-{}-GAN/'.format(L,L)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "G = Generator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "\n",
    "max_epoch = 100\n",
    "step = 0\n",
    "n_noise = 100 # noise dimension\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# We will denote real images as 1s and fake images as 0s\n",
    "# This is why we needed to drop the last batch of the data loader\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, D_loss: 1.3866207599639893, G_loss: -0.6661349534988403\n",
      "Epoch: 3, Step: 100, D_loss: 1.2046635150909424, G_loss: -0.45369499921798706\n",
      "Epoch: 6, Step: 200, D_loss: 1.4597392082214355, G_loss: -0.6861318349838257\n",
      "Epoch: 9, Step: 300, D_loss: 1.569345474243164, G_loss: -0.795249879360199\n",
      "Epoch: 12, Step: 400, D_loss: 1.3305789232254028, G_loss: -0.701115608215332\n",
      "Epoch: 15, Step: 500, D_loss: 1.3233239650726318, G_loss: -0.6196242570877075\n",
      "Epoch: 18, Step: 600, D_loss: 1.2953805923461914, G_loss: -0.42573338747024536\n",
      "Epoch: 21, Step: 700, D_loss: 1.2323169708251953, G_loss: -0.4161761403083801\n",
      "Epoch: 25, Step: 800, D_loss: 1.1252758502960205, G_loss: -0.8300338983535767\n",
      "Epoch: 28, Step: 900, D_loss: 1.0256104469299316, G_loss: -0.6917607188224792\n",
      "Epoch: 31, Step: 1000, D_loss: 0.9961529970169067, G_loss: -0.24637596309185028\n",
      "Epoch: 34, Step: 1100, D_loss: 0.4614802896976471, G_loss: -0.20906482636928558\n",
      "Epoch: 37, Step: 1200, D_loss: 0.6606758236885071, G_loss: -0.18568958342075348\n",
      "Epoch: 40, Step: 1300, D_loss: 0.546105146408081, G_loss: -0.21768568456172943\n",
      "Epoch: 43, Step: 1400, D_loss: 0.5834276080131531, G_loss: -0.19482770562171936\n",
      "Epoch: 46, Step: 1500, D_loss: 0.4908429682254791, G_loss: -0.279175341129303\n",
      "Epoch: 50, Step: 1600, D_loss: 0.21373043954372406, G_loss: -0.08950196206569672\n",
      "Epoch: 53, Step: 1700, D_loss: 0.2538975477218628, G_loss: -0.129543274641037\n",
      "Epoch: 56, Step: 1800, D_loss: 0.2516367435455322, G_loss: -0.213276669383049\n",
      "Epoch: 59, Step: 1900, D_loss: 0.14190059900283813, G_loss: -0.07839169353246689\n",
      "Epoch: 62, Step: 2000, D_loss: 0.041344642639160156, G_loss: -0.005008500535041094\n",
      "Epoch: 65, Step: 2100, D_loss: 0.23018240928649902, G_loss: -0.034497231245040894\n",
      "Epoch: 68, Step: 2200, D_loss: 0.132671058177948, G_loss: -0.029005905613303185\n",
      "Epoch: 71, Step: 2300, D_loss: 0.31499335169792175, G_loss: -0.5467456579208374\n",
      "Epoch: 75, Step: 2400, D_loss: 0.011543799191713333, G_loss: -0.00425213947892189\n",
      "Epoch: 78, Step: 2500, D_loss: 0.006105509120970964, G_loss: -0.0013752850936725736\n",
      "Epoch: 81, Step: 2600, D_loss: 0.0052330344915390015, G_loss: -0.0011598265264183283\n",
      "Epoch: 84, Step: 2700, D_loss: 0.007194272242486477, G_loss: -0.0017239103326573968\n",
      "Epoch: 87, Step: 2800, D_loss: 0.26286476850509644, G_loss: -0.06514450907707214\n",
      "Epoch: 90, Step: 2900, D_loss: 0.3135516047477722, G_loss: -0.11807078123092651\n",
      "Epoch: 93, Step: 3000, D_loss: 0.7509583234786987, G_loss: -0.24475660920143127\n",
      "Epoch: 96, Step: 3100, D_loss: 2.463230609893799, G_loss: -0.23343728482723236\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAMPLE_PATH):\n",
    "    os.makedirs(SAMPLE_PATH)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Train Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        y = labels.view(batch_size, 1) # add singleton dimension so batch_size x 1\n",
    "        y = to_onehot(y, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        G_loss = -1 * criterion(z_outputs, D_fakes)\n",
    "\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: {}, Step: {}, D_loss: {}, G_loss: {}'.format(epoch, step, D_loss.item(), G_loss.item()))\n",
    "            \n",
    "        if step % 500 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, DEVICE, n_noise)\n",
    "            plt.imsave(SAMPLE_PATH + 'sample-{}-{}.png'.format(epoch, step), img, cmap='gray')\n",
    "            G.train()\n",
    "\n",
    "        step += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(G, device, n_noise, label):\n",
    "    G.eval()\n",
    "    c = torch.zeros([1, num_classes]).to(device)\n",
    "    c[:, label] = 1\n",
    "    z = torch.randn(1, n_noise).to(device)\n",
    "    y_hat = G(z,c).view(1, L, L)\n",
    "    y_hat = normalize_generator_output(y_hat)\n",
    "    # print(y_hat)\n",
    "    result = y_hat.cpu().data.numpy()\n",
    "    G.train()\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42796aa310>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = 0\n",
    "temp = train_data.classes[label]\n",
    "print(temp)\n",
    "img = generate_image(G, DEVICE, n_noise, 0)\n",
    "# print(img.shape)\n",
    "# print(img)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42797b3070>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "label = 18\n",
    "temp = train_data.classes[label]\n",
    "print(temp)\n",
    "img = generate_image(G, DEVICE, n_noise, 15)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(G, MODEL_PATH+'G.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
