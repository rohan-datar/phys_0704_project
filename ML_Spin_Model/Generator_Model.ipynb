{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spin Configuration Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "You can load different datasets by changing the dataset path for testing, trainging, and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to the appropriate directory paths\n",
    "\n",
    "train_dir = '../data/{}-{}/temp_class/train/'.format(L,L)\n",
    "\n",
    "# Set these variables to the appropriate directory paths\n",
    "benchmark_dir = '../data/{}-{}/temp_class/test/'.format(L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms\n",
    "dset_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "#create datasets using ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=dset_transform)\n",
    "# test_data = datasets.ImageFolder(test_dir, transform=dset_transform)\n",
    "benchmark_data = datasets.ImageFolder(benchmark_dir, transform=dset_transform)\n",
    "\n",
    "batch_size = 64\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_data.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Thanks to Teal Witter for code for these helper functions https://github.com/rtealwitter/dl-demos/blob/b537a5dd94953ea656a2140227af78f67c042540/demo11-conditional-gan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_generator_output(x):\n",
    "    #convert to binary data\n",
    "    x[x>=0.5] = 1\n",
    "    x[x<0.5] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=2):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, DEVICE, n_noise=100):\n",
    "    img = np.zeros([L*num_classes, L*num_classes])\n",
    "    for j in range(num_classes):\n",
    "        c = torch.zeros([num_classes, num_classes]).to(DEVICE)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(num_classes, n_noise).to(DEVICE)\n",
    "        y_hat = G(z,c).view(num_classes, L, L)\n",
    "        y_hat = normalize_generator_output(y_hat)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*L:(j+1)*L] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [L,L]\n",
    "hidden_layers = 10\n",
    "layer_size = 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Define the architecture of the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(layer_size, num_layers):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(layer_size, layer_size))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes, input_size=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(input_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, self.image_size), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(self.image_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, 1), nn.Sigmoid()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/{}-{}-GAN/'.format(L,L)\n",
    "SAMPLE_PATH = '../generated_samples/{}-{}-GAN/'.format(L,L)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "G = Generator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "\n",
    "max_epoch = 30\n",
    "step = 0\n",
    "n_noise = 100 # noise dimension\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# We will denote real images as 1s and fake images as 0s\n",
    "# This is why we needed to drop the last batch of the data loader\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, D_loss: 1.3865723609924316, G_loss: -0.6802883148193359\n",
      "Epoch: 3, Step: 100, D_loss: 1.3300395011901855, G_loss: -0.5295279622077942\n",
      "Epoch: 6, Step: 200, D_loss: 1.6028714179992676, G_loss: -0.7912464737892151\n",
      "Epoch: 9, Step: 300, D_loss: 1.350998878479004, G_loss: -0.5542240142822266\n",
      "Epoch: 12, Step: 400, D_loss: 1.367965817451477, G_loss: -0.6836874485015869\n",
      "Epoch: 15, Step: 500, D_loss: 1.3766212463378906, G_loss: -0.6562753915786743\n",
      "Epoch: 18, Step: 600, D_loss: 1.35806405544281, G_loss: -0.6356407999992371\n",
      "Epoch: 21, Step: 700, D_loss: 1.3853201866149902, G_loss: -0.7266126871109009\n",
      "Epoch: 25, Step: 800, D_loss: 1.3748140335083008, G_loss: -0.6746338605880737\n",
      "Epoch: 28, Step: 900, D_loss: 1.405277967453003, G_loss: -0.383297860622406\n",
      "Epoch: 31, Step: 1000, D_loss: 1.2315599918365479, G_loss: -0.5882102251052856\n",
      "Epoch: 34, Step: 1100, D_loss: 1.261582612991333, G_loss: -0.5381132364273071\n",
      "Epoch: 37, Step: 1200, D_loss: 1.164994716644287, G_loss: -0.5579310655593872\n",
      "Epoch: 40, Step: 1300, D_loss: 1.083130121231079, G_loss: -0.5103403329849243\n",
      "Epoch: 43, Step: 1400, D_loss: 1.208406925201416, G_loss: -0.4364292025566101\n",
      "Epoch: 46, Step: 1500, D_loss: 1.2401622533798218, G_loss: -0.5996408462524414\n",
      "Epoch: 50, Step: 1600, D_loss: 1.3027549982070923, G_loss: -0.7919946908950806\n",
      "Epoch: 53, Step: 1700, D_loss: 1.1425042152404785, G_loss: -0.5577478408813477\n",
      "Epoch: 56, Step: 1800, D_loss: 1.1985528469085693, G_loss: -0.5646702647209167\n",
      "Epoch: 59, Step: 1900, D_loss: 1.309777855873108, G_loss: -0.4870431423187256\n",
      "Epoch: 62, Step: 2000, D_loss: 1.2175333499908447, G_loss: -0.6119277477264404\n",
      "Epoch: 65, Step: 2100, D_loss: 1.2428865432739258, G_loss: -0.6131646633148193\n",
      "Epoch: 68, Step: 2200, D_loss: 1.3666069507598877, G_loss: -0.697310209274292\n",
      "Epoch: 71, Step: 2300, D_loss: 1.392143964767456, G_loss: -0.5466028451919556\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAMPLE_PATH):\n",
    "    os.makedirs(SAMPLE_PATH)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Train Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        y = labels.view(batch_size, 1) # add singleton dimension so batch_size x 1\n",
    "        y = to_onehot(y, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        G_loss = -1 * criterion(z_outputs, D_fakes)\n",
    "\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: {}, Step: {}, D_loss: {}, G_loss: {}'.format(epoch, step, D_loss.item(), G_loss.item()))\n",
    "            \n",
    "        if step % 500 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, DEVICE, n_noise)\n",
    "            plt.imsave(SAMPLE_PATH + 'sample-{}-{}.png'.format(epoch, step), img, cmap='gray')\n",
    "            G.train()\n",
    "\n",
    "        step += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(G, device, n_noise, label):\n",
    "    G.eval()\n",
    "    c = torch.zeros([1, num_classes]).to(device)\n",
    "    c[:, label] = 1\n",
    "    z = torch.randn(1, n_noise).to(device)\n",
    "    y_hat = G(z,c).view(1, L, L)\n",
    "    y_hat = normalize_generator_output(y_hat)\n",
    "    # print(y_hat)\n",
    "    result = y_hat.cpu().data.numpy()\n",
    "    G.train()\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e316cbc40>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = 0\n",
    "temp = train_data.classes[label]\n",
    "print(temp)\n",
    "img = generate_image(G, DEVICE, n_noise, 0)\n",
    "# print(img.shape)\n",
    "# print(img)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e316b3850>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "label = 20\n",
    "temp = train_data.classes[label]\n",
    "print(temp)\n",
    "img = generate_image(G, DEVICE, n_noise, 15)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(G, MODEL_PATH+'G.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
