{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spin Configuration Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "You can load different datasets by changing the dataset path for testing, trainging, and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to the appropriate directory paths\n",
    "\n",
    "test_dir = '../data/{}-{}/binary_class/test/'.format(L,L)\n",
    "train_dir = '../data/{}-{}/binary_class/train/'.format(L,L)\n",
    "\n",
    "# Set these variables to the appropriate directory paths\n",
    "benchmark_dir = '../data/{}-{}/temp_class/test/'.format(L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms\n",
    "dset_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "#create datasets using ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=dset_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=dset_transform)\n",
    "benchmark_data = datasets.ImageFolder(benchmark_dir, transform=dset_transform)\n",
    "\n",
    "batch_size = 64\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Thanks to Teal Witter for code for these helper functions https://github.com/rtealwitter/dl-demos/blob/b537a5dd94953ea656a2140227af78f67c042540/demo11-conditional-gan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=2):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, DEVICE, n_noise=100):\n",
    "    img = np.zeros([L*2, L*2])\n",
    "    for j in range(2):\n",
    "        c = torch.zeros([2, 2]).to(DEVICE)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(2, n_noise).to(DEVICE)\n",
    "        y_hat = G(z,c).view(2, L, L)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*L:(j+1)*L] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [L,L]\n",
    "hidden_layers = 4\n",
    "layer_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Define the architecture of the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(layer_size, num_layers):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(layer_size, layer_size))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes, input_size=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(input_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, self.image_size), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim, hidden_layers, layer_size, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = np.prod(image_dim)\n",
    "        self.input_layer = [nn.Linear(self.image_size + num_classes, layer_size)]\n",
    "        self.hidden_layers = linear_block(layer_size, hidden_layers)\n",
    "        self.output_layer = [nn.Linear(layer_size, 1), nn.Sigmoid()]\n",
    "        self.model = nn.Sequential(*self.input_layer, *self.hidden_layers, *self.output_layer)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat([x, c], dim=1)\n",
    "        return self.model(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/{}-{}-GAN/'.format(L,L)\n",
    "SAMPLE_PATH = '../generated_samples/{}-{}-GAN/'.format(L,L)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "G = Generator(input_size, hidden_layers, layer_size, num_classes).to(DEVICE)\n",
    "\n",
    "max_epoch = 50\n",
    "step = 0\n",
    "n_noise = 100 # noise dimension\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# We will denote real images as 1s and fake images as 0s\n",
    "# This is why we needed to drop the last batch of the data loader\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, D_loss: 1.3846461772918701, G_loss: -0.6676919460296631\n",
      "Epoch: 1, Step: 100, D_loss: 2.182219982147217, G_loss: -1.258021593093872\n",
      "Epoch: 3, Step: 200, D_loss: 1.327312707901001, G_loss: -0.327745258808136\n",
      "Epoch: 4, Step: 300, D_loss: 1.253021478652954, G_loss: -0.46114084124565125\n",
      "Epoch: 6, Step: 400, D_loss: 1.2940622568130493, G_loss: -0.444781631231308\n",
      "Epoch: 7, Step: 500, D_loss: 1.4484095573425293, G_loss: -0.634724497795105\n",
      "Epoch: 9, Step: 600, D_loss: 1.3560107946395874, G_loss: -0.5406897068023682\n",
      "Epoch: 10, Step: 700, D_loss: 1.2805793285369873, G_loss: -0.5863516330718994\n",
      "Epoch: 12, Step: 800, D_loss: 1.2971081733703613, G_loss: -0.6525602340698242\n",
      "Epoch: 14, Step: 900, D_loss: 1.1712019443511963, G_loss: -0.4854898154735565\n",
      "Epoch: 15, Step: 1000, D_loss: 1.2363122701644897, G_loss: -0.7394949793815613\n",
      "Epoch: 17, Step: 1100, D_loss: 1.1473584175109863, G_loss: -0.4680593013763428\n",
      "Epoch: 18, Step: 1200, D_loss: 1.277485728263855, G_loss: -0.5198801755905151\n",
      "Epoch: 20, Step: 1300, D_loss: 1.2135372161865234, G_loss: -0.35215264558792114\n",
      "Epoch: 21, Step: 1400, D_loss: 1.2705923318862915, G_loss: -1.0233999490737915\n",
      "Epoch: 23, Step: 1500, D_loss: 1.3521921634674072, G_loss: -0.5805830359458923\n",
      "Epoch: 25, Step: 1600, D_loss: 0.8463186621665955, G_loss: -0.19881707429885864\n",
      "Epoch: 26, Step: 1700, D_loss: 0.7986837029457092, G_loss: -0.37127935886383057\n",
      "Epoch: 28, Step: 1800, D_loss: 0.8990738391876221, G_loss: -0.5497642755508423\n",
      "Epoch: 29, Step: 1900, D_loss: 1.0738062858581543, G_loss: -0.44307631254196167\n",
      "Epoch: 31, Step: 2000, D_loss: 0.6705337762832642, G_loss: -0.24692600965499878\n",
      "Epoch: 32, Step: 2100, D_loss: 0.9221330881118774, G_loss: -0.4741867482662201\n",
      "Epoch: 34, Step: 2200, D_loss: 0.7900263667106628, G_loss: -0.17150592803955078\n",
      "Epoch: 35, Step: 2300, D_loss: 0.7871803641319275, G_loss: -0.35101526975631714\n",
      "Epoch: 37, Step: 2400, D_loss: 0.5542103052139282, G_loss: -0.25079119205474854\n",
      "Epoch: 39, Step: 2500, D_loss: 0.7084003686904907, G_loss: -0.2715204358100891\n",
      "Epoch: 40, Step: 2600, D_loss: 0.578770637512207, G_loss: -0.16890686750411987\n",
      "Epoch: 42, Step: 2700, D_loss: 0.4508989453315735, G_loss: -0.246900737285614\n",
      "Epoch: 43, Step: 2800, D_loss: 0.7450011968612671, G_loss: -0.358928382396698\n",
      "Epoch: 45, Step: 2900, D_loss: 0.6125397682189941, G_loss: -0.38704848289489746\n",
      "Epoch: 46, Step: 3000, D_loss: 1.332811951637268, G_loss: -0.24533265829086304\n",
      "Epoch: 48, Step: 3100, D_loss: 0.5375757813453674, G_loss: -0.2614824175834656\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAMPLE_PATH):\n",
    "    os.makedirs(SAMPLE_PATH)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Train Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        y = labels.view(batch_size, 1) # add singleton dimension so batch_size x 1\n",
    "        y = to_onehot(y, num_classes=2).to(DEVICE)\n",
    "\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        G_loss = -1 * criterion(z_outputs, D_fakes)\n",
    "\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: {}, Step: {}, D_loss: {}, G_loss: {}'.format(epoch, step, D_loss.item(), G_loss.item()))\n",
    "            \n",
    "        if step % 500 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, DEVICE, n_noise)\n",
    "            plt.imsave(SAMPLE_PATH + 'sample-{}-{}.png'.format(epoch, step), img, cmap='gray')\n",
    "            G.train()\n",
    "\n",
    "        step += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
